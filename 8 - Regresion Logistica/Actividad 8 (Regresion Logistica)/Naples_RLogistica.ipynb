{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as special\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) host_response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\thost_response_time\n",
    "•\thost_acceptance_rate\n",
    "•\thost_is_superhost\n",
    "•\tnumber_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sin especificar' 'within a few hours' 'within an hour'\n",
      " 'a few days or more' 'within a day']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_response_time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_response_time\n",
      "Rápido    7972\n",
      "Lento     3055\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_response_time a dicotomica\n",
    "Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n",
    "    \"within an hour\": \"Rápido\",\n",
    "    \"within a few hours\": \"Rápido\",\n",
    "    \"within a day\": \"Lento\",\n",
    "    \"a few days or more\": \"Lento\",\n",
    "    \"Sin especificar\": \"Lento\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_response_time\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f' 'Sin especificar' 't']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_is_superhost\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\1419231691.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\"f\": 0, \"t\": 1, \"Sin especificar\": 2})\n"
     ]
    }
   ],
   "source": [
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\"f\": 0, \"t\": 1, \"Sin especificar\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_is_superhost\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_acceptance_rate', 'host_is_superhost', 'number_of_reviews']]\n",
    "Var_Dep = Naples['host_response_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lento', 'Rápido', 'Rápido', ..., 'Rápido', 'Rápido', 'Rápido'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 391  513]\n",
      " [ 160 2245]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.8139956490210297\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# rapido o lento - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Rápido\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.7966152916288909\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.4325221238938053\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Lento\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) host_is_superhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\thost_is_superhost\n",
    "•\t// No lo use host_response_rate (todo es 1)\n",
    "•\thost_acceptance_rate\n",
    "•\thost_total_listings_count\n",
    "•\thost_response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sin especificar' 'within a few hours' 'within an hour'\n",
      " 'a few days or more' 'within a day']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_response_time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "No Superhost    8202\n",
      "Superhost       2825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n",
    "    \"Sin especificar\": \"No Superhost\",\n",
    "    \"f\": \"No Superhost\",\n",
    "    \"t\": \"Superhost\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_response_time\n",
      "1    7330\n",
      "5    2468\n",
      "2     642\n",
      "3     393\n",
      "4     194\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\2485131506.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_response_time a dicotomica\n",
    "Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n",
    "    \"within an hour\": 1,\n",
    "    \"within a few hours\": 2,\n",
    "    \"within a day\": 3,\n",
    "    \"a few days or more\": 4,\n",
    "    \"Sin especificar\": 5\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_response_time\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_response_time', 'host_acceptance_rate', 'host_total_listings_count']]\n",
    "Var_Dep = Naples['host_is_superhost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No Superhost', 'No Superhost', 'No Superhost', ...,\n",
       "       'No Superhost', 'No Superhost', 'No Superhost'], dtype=object)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[2440    0]\n",
      " [ 869    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bondleton\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# super o no super - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Superhost\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.7373828951344817\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"No Superhost\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) beds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tbeds\n",
    "•\tAccommodates\n",
    "•\tBedrooms\n",
    "•\tRoom_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  3.  1.4 2.  1.3 0. ]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"bedrooms\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beds\n",
      "Con cama    10778\n",
      "Sin cama      249\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"beds\"] = Naples[\"beds\"].replace({\n",
    "    0: \"Sin cama\",\n",
    "    6: \"Con cama\",\n",
    "    5: \"Con cama\",\n",
    "    4: \"Con cama\",\n",
    "    3: \"Con cama\",\n",
    "    2.2: \"Con cama\",\n",
    "    2.1: \"Con cama\",\n",
    "    2: \"Con cama\",  \n",
    "    1: \"Con cama\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"beds\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Private room' 'Entire home/apt' 'Hotel room' 'Shared room']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"room_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "1    7195\n",
      "2    3621\n",
      "4     128\n",
      "3      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\12446977.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": 1,  # Privado\n",
    "    \"Private room\": 2,      # Privado\n",
    "    \"Shared room\": 3,       # Compartido\n",
    "    \"Hotel room\": 4         # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['accommodates', 'bedrooms', 'room_type']]\n",
    "Var_Dep = Naples['beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Con cama', 'Con cama', 'Con cama', ..., 'Con cama', 'Con cama',\n",
       "       'Con cama'], dtype=object)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[3237    0]\n",
      " [  72    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9782411604714415\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cama o sin cama - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Con cama\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9782411604714415\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Sin cama\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) host_has_profile_pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\thost_has_profile_pic\n",
    "•\thost_listings_count\n",
    "•\thost_verifications\n",
    "•\tHost_is_superhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t' 'f']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_has_profile_pic\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_has_profile_pic\n",
      "Con foto    9997\n",
      "Sin foto    1030\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Convertir host_has_profile_pic a dicotomica\n",
    "# Naples[\"host_has_profile_pic\"] = Naples[\"host_has_profile_pic\"].replace({\n",
    "#     \"t\": \"Con foto\",\n",
    "#     \"f\": \"Sin foto\",\n",
    "# })\n",
    "\n",
    "# # Verificar que la conversión fue correcta\n",
    "# print(Naples[\"host_has_profile_pic\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_verifications\n",
      "1    9342\n",
      "3    1193\n",
      "2     468\n",
      "4      11\n",
      "6       7\n",
      "5       6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_verifications a dicotomica\n",
    "Naples[\"host_verifications\"] = Naples[\"host_verifications\"].replace({\n",
    "    \"['email', 'phone']\": 1, # Verificado = 1\n",
    "    \"['email', 'phone', 'work_email']\": 2,\n",
    "    \"['phone']\": 3,\n",
    "    \"['email']\": 4,\n",
    "    \"['phone', 'work_email']\": 5,\n",
    "    \"[]\": \"6\", # No verificado = 0\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_verifications\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "0    8202\n",
      "1    2825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n",
    "    \"Sin especificar\": \"0\",\n",
    "    \"f\": \"0\",\n",
    "    \"t\": \"1\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_listings_count', 'host_verifications', 'host_is_superhost']]\n",
    "Var_Dep = Naples['host_has_profile_pic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['t', 't', 't', ..., 't', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[   2  324]\n",
      " [   3 2980]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9019370460048426\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9011786038077969\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.006134969325153374\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) host_identity_verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\thost_identity_verified\n",
    "•\tHost_is_superhost\n",
    "•\treview_scores_communication\n",
    "•\treview_scores_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f' 'Sin especificar' 't']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_is_superhost\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_is_superhost\n",
      "2    7814\n",
      "1    2825\n",
      "3     388\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_is_superhost a dicotomica\n",
    "Naples[\"host_is_superhost\"] = Naples[\"host_is_superhost\"].replace({\n",
    "    \"Sin especificar\": \"3\",\n",
    "    \"f\": \"2\",\n",
    "    \"t\": \"1\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_is_superhost\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['host_is_superhost', 'review_scores_communication', 'review_scores_rating']]\n",
    "Var_Dep = Naples['host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['t', 't', 't', ..., 't', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[   0  238]\n",
      " [   0 3071]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9280749471139317\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9280749471139317\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) room_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\troom_type\n",
    "•\tproperty_type\n",
    "•\taccommodates\n",
    "•\tprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 71.   76.   68.   58.   60.   42.   93.3 117.6  90.  100.   91.   35.\n",
      "  99.  103.   75.   62.   70.   80.  105.  140.   55.  124.   85.   53.\n",
      "  82.  110.   46.   40.  159.  120.  109.  126.   20.  102.   89.  183.\n",
      "  94.   83.   61.   63.   57.   78.  127.   65.   50.   74.  160.  187.\n",
      " 145.   48.   95.  164.   43.  113.   73.   86.  139.   64.  175.  144.\n",
      " 119.  148.  190.   72.   30.  123.  135.   92.   38.  153.   47.   67.\n",
      "  45.   93.  114.   97.   66.   59.   96.  150.   28.   31.  118.  104.\n",
      " 193.   39.  156.  106.   22.  180.   79.  128.  151.   81.   84.   23.\n",
      " 107.  162.  101.  147.  167.  132.  115.  166.   88.   27.  174.  130.\n",
      "  69.  170.   33.  178.   52.   77.  161.   54.  158.   87.   37.   98.\n",
      " 125.  122.  186.  169.  179.  108.  117.   56.  195.  177.  129.  116.\n",
      "  36.  134.  111.  168.   13.  194.   18.  112.  121.   51.   26.  146.\n",
      "  34.  184.  141.  155.   49.  131.  137.  163.  173.   32.  149.  133.\n",
      "  29.  136.   44.  191.  152.   21.  142.  154.  189.  188.  143.  176.\n",
      " 192.  165.   14.   25.   15.   41.  138.  172.  171.  182.  157.  185.\n",
      " 181.   16.   17. ]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"price\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "Privado       10816\n",
      "Compartido      211\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": \"Privado\",  # Privado\n",
    "    \"Private room\": \"Privado\",      # Privado\n",
    "    \"Shared room\": \"Compartido\",       # Compartido\n",
    "    \"Hotel room\": \"Compartido\"        # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property_type\n",
      "3     4095\n",
      "1     1690\n",
      "9     1366\n",
      "4     1093\n",
      "7      921\n",
      "12     451\n",
      "2      311\n",
      "14     142\n",
      "5      110\n",
      "8      102\n",
      "24     102\n",
      "11      99\n",
      "30      66\n",
      "15      58\n",
      "33      57\n",
      "26      54\n",
      "31      42\n",
      "44      39\n",
      "35      28\n",
      "28      23\n",
      "27      20\n",
      "13      17\n",
      "29      17\n",
      "10      14\n",
      "23      13\n",
      "18      11\n",
      "48      10\n",
      "45       9\n",
      "21       9\n",
      "39       7\n",
      "36       6\n",
      "42       4\n",
      "37       4\n",
      "17       4\n",
      "51       3\n",
      "20       3\n",
      "32       3\n",
      "52       3\n",
      "22       2\n",
      "43       2\n",
      "38       2\n",
      "34       2\n",
      "16       2\n",
      "19       2\n",
      "41       1\n",
      "40       1\n",
      "6        1\n",
      "46       1\n",
      "47       1\n",
      "49       1\n",
      "50       1\n",
      "25       1\n",
      "53       1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\3781815957.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"property_type\"] = Naples[\"property_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Crear diccionario con valores numéricos para cada tipo de propiedad\n",
    "Naples[\"property_type\"] = Naples[\"property_type\"].replace({\n",
    "    'Private room in bed and breakfast': 1,\n",
    "    'Private room in condo': 2,\n",
    "    'Entire rental unit': 3,\n",
    "    'Private room in rental unit': 4,\n",
    "    'Entire loft': 5,\n",
    "    'Entire cottage': 6,\n",
    "    'Entire home': 7,\n",
    "    'Room in bed and breakfast': 8,\n",
    "    'Entire condo': 9,\n",
    "    'Shared room in hostel': 10,\n",
    "    'Room in boutique hotel': 11,\n",
    "    'Entire vacation home': 12,\n",
    "    'Entire villa': 13,\n",
    "    'Private room in home': 14,\n",
    "    'Entire serviced apartment': 15,\n",
    "    'Entire townhouse': 16,\n",
    "    'Private room in hostel': 17,\n",
    "    'Private room': 18,\n",
    "    'Entire bed and breakfast': 19,\n",
    "    'Private room in villa': 20,\n",
    "    'Entire guesthouse': 21,\n",
    "    'Room in hostel': 22,\n",
    "    'Boat': 23,\n",
    "    'Tiny home': 24,\n",
    "    'Dome': 25,\n",
    "    'Room in hotel': 26,\n",
    "    'Shared room in bed and breakfast': 27,\n",
    "    'Entire guest suite': 28,\n",
    "    'Room in serviced apartment': 29,\n",
    "    'Private room in vacation home': 30,\n",
    "    'Shared room in rental unit': 31,\n",
    "    'Shared room in condo': 32,\n",
    "    'Private room in serviced apartment': 33,\n",
    "    'Entire place': 34,\n",
    "    'Private room in guest suite': 35,\n",
    "    'Private room in tiny home': 36,\n",
    "    'Private room in townhouse': 37,\n",
    "    'Private room in boat': 38,\n",
    "    'Private room in loft': 39,\n",
    "    'Earthen home': 40,\n",
    "    'Dammuso': 41,\n",
    "    'Shared room in home': 42,\n",
    "    'Houseboat': 43,\n",
    "    'Private room in guesthouse': 44,\n",
    "    'Private room in casa particular': 45,\n",
    "    'Camper/RV': 46,\n",
    "    'Castle': 47,\n",
    "    'Casa particular': 48,\n",
    "    'Entire home/apt': 49,\n",
    "    'Farm stay': 50,\n",
    "    'Private room in farm stay': 51,\n",
    "    'Room in aparthotel': 52,\n",
    "    'Tent': 53\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"property_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['property_type', 'accommodates', 'price']]\n",
    "Var_Dep = Naples['room_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Privado', 'Privado', 'Privado', ..., 'Privado', 'Privado',\n",
       "       'Privado'], dtype=object)"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[   0   65]\n",
      " [   0 3244]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9803566032033847\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cama o sin cama - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Privado\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9803566032033847\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Compartido\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) has_availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\thas_availability\n",
    "•\tprice\n",
    "•\tavailability_365\n",
    "•\tnumber_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Disponible' 'No disponible']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"has_availability\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_availability\n",
      "Disponible       10994\n",
      "No disponible       33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir\n",
    "Naples[\"has_availability\"] = Naples[\"has_availability\"].replace({\n",
    "    \"t\": \"Disponible\",\n",
    "    \"f\": \"No disponible\",\n",
    "    \"Sin especificar\": \"No disponible\",\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"has_availability\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['price', 'availability_365', 'number_of_reviews']]\n",
    "Var_Dep = Naples['has_availability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Disponible', 'Disponible', 'Disponible', ..., 'Disponible',\n",
       "       'Disponible', 'Disponible'], dtype=object)"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[3304    0]\n",
      " [   5    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.9984889694771835\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Disponible\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.9984889694771835\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"No disponible\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) instant_bookable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tinstant_bookable\n",
    "•\tmaximum_nights\n",
    "•\tminimum_nights\n",
    "•\thost_response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sin especificar' 'within a few hours' 'within an hour'\n",
      " 'a few days or more' 'within a day']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"host_response_time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_response_time\n",
      "1    7330\n",
      "5    2468\n",
      "2     642\n",
      "3     393\n",
      "4     194\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\2485131506.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir host_response_time a dicotomica\n",
    "Naples[\"host_response_time\"] = Naples[\"host_response_time\"].replace({\n",
    "    \"within an hour\": 1,\n",
    "    \"within a few hours\": 2,\n",
    "    \"within a day\": 3,\n",
    "    \"a few days or more\": 4,\n",
    "    \"Sin especificar\": 5\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"host_response_time\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['maximum_nights', 'minimum_nights', 'host_response_time']]\n",
    "Var_Dep = Naples['instant_bookable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['f', 't', 't', ..., 't', 't', 't'], dtype=object)"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 478  950]\n",
      " [ 342 1539]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.6183206106870229\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# rapido o lento - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"t\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.6095497129042007\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.33473389355742295\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"f\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) bedrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tbedrooms\n",
    "•\troom_type\n",
    "•\tbathrooms\n",
    "•\tbeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  2.  3.  2.2 2.1 6.  4.  5.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"beds\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedrooms\n",
      "Con cuarto    10834\n",
      "Sin cuarto      193\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir bedrooms a dicotomica\n",
    "Naples[\"bedrooms\"] = Naples[\"bedrooms\"].replace({\n",
    "    0: \"Sin cuarto\",\n",
    "    1: \"Con cuarto\",\n",
    "    1.3: \"Con cuarto\",\n",
    "    1.4: \"Con cuarto\",\n",
    "    2: \"Con cuarto\",\n",
    "    3: \"Con cuarto\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"bedrooms\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "1    7195\n",
      "2    3621\n",
      "4     128\n",
      "3      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\12446977.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": 1,  # Privado\n",
    "    \"Private room\": 2,      # Privado\n",
    "    \"Shared room\": 3,       # Compartido\n",
    "    \"Hotel room\": 4         # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['room_type', 'bathrooms', 'beds']]\n",
    "Var_Dep = Naples['bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Con cuarto', 'Con cuarto', 'Con cuarto', ..., 'Con cuarto',\n",
       "       'Con cuarto', 'Con cuarto'], dtype=object)"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[3245    0]\n",
      " [  64    0]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.980658809307948\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cuarto o sin cuarto - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Con cuarto\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.980658809307948\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Sin cuarto\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) minimum_nights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tminimum_nights\n",
    "•\tprice\n",
    "•\troom_type\n",
    "•\tavailability_365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar archivo desde equipo\n",
    "Naples = pd.read_csv(\"Datos_limpios_Naples.csv\")\n",
    "Naples = Naples.drop(['Unnamed: 0'], axis=1)\n",
    "# Naples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Private room' 'Entire home/apt' 'Hotel room' 'Shared room']\n"
     ]
    }
   ],
   "source": [
    "print(Naples[\"room_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum_nights\n",
      "Noche     6072\n",
      "Noches    4955\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir minimum_nights a dicotomica\n",
    "Naples[\"minimum_nights\"] = Naples[\"minimum_nights\"].replace({\n",
    "    1: \"Noche\",\n",
    "    1.5: \"Noche\",\n",
    "    2: \"Noches\",\n",
    "    3: \"Noches\"\n",
    "})\n",
    "\n",
    "# Verificar que la conversión fue correcta\n",
    "print(Naples[\"minimum_nights\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_type\n",
      "1    7195\n",
      "2    3621\n",
      "4     128\n",
      "3      83\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bondleton\\AppData\\Local\\Temp\\ipykernel_28192\\12446977.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n"
     ]
    }
   ],
   "source": [
    "# Convertir room_type a binario (Privado vs. Compartido)\n",
    "Naples[\"room_type\"] = Naples[\"room_type\"].replace({\n",
    "    \"Entire home/apt\": 1,  # Privado\n",
    "    \"Private room\": 2,      # Privado\n",
    "    \"Shared room\": 3,       # Compartido\n",
    "    \"Hotel room\": 4         # Compartido\n",
    "})\n",
    "\n",
    "# Confirmar conversión\n",
    "print(Naples[\"room_type\"].value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables dependientes e independientes para la regresion logistica\n",
    "Vars_Indep = Naples[['price', 'room_type', 'availability_365']]\n",
    "Var_Dep = Naples['minimum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinimos las variables, arreglo matricial\n",
    "X = Vars_Indep\n",
    "y = Var_Dep\n",
    "\n",
    "# Conjunto de entrenamienso se ingresa y da un salida con respecto \n",
    "# Dividimos  el conjunto de datos en la parte de entrenamiento (70%) y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Escalar todos los datos\n",
    "escalar = StandardScaler()\n",
    "\n",
    "# Para realizar el escalamiento de las variables x  tanto de entrenamiento como de prueba utilizamos \n",
    "# La y es solo do, y las dependeintes si se pueden escalar\n",
    "X_train = escalar.fit_transform(X_train)\n",
    "X_test = escalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Noche', 'Noche', 'Noches', ..., 'Noche', 'Noches', 'Noches'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el algoritmo a utilizar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "algoritmo = LogisticRegression()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos las predicciones\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      "[[ 926  927]\n",
      " [ 368 1088]]\n"
     ]
    }
   ],
   "source": [
    "# Verifico la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:')\n",
    "print(matriz)\n",
    "\n",
    "# diagonal: \n",
    "# diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo:\n",
      "0.7156105100463679\n"
     ]
    }
   ],
   "source": [
    "# calculo precision del modelo \n",
    "# con cuarto o sin cuarto - pos_label\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label=\"Noche\")\n",
    "print('Precisión del modelo:')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo:\n",
      "0.6086430945905107\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la exactitud del modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print('Exactitud del modelo:')\n",
    "print(exactitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensibilidad del modelo:\n",
      "0.7472527472527473\n"
     ]
    }
   ],
   "source": [
    "# Calculo de la sensibilidad del modelo\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Noches\")\n",
    "print('Sensibilidad del modelo:')\n",
    "print(sensibilidad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
